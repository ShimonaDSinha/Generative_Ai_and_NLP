{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296bc5ff-606f-4669-9789-1a87355febe1",
   "metadata": {},
   "source": [
    "Name:Shimona Sinha\n",
    "\n",
    "SRN:PES2UG23CS547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9267a3-3339-4ef4-92bb-977b1815ef6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73c8e5-3d01-4d2a-beaa-42a5892421f5",
   "metadata": {},
   "source": [
    "**EXPERIMENT 1: TEXT GENERATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d00bcc-f630-4f89-b1fe-81d846b34ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1A. BERT – Text Generation\n",
    "gen_bert = pipeline(\"text-generation\", model=\"bert-base-uncased\")\n",
    "gen_bert(\"The future of Artificial Intelligence is\", max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea9f28a2-5fd8-4621-93f7-6d5e2a2689ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1B. RoBERTa – Text Generation\n",
    "gen_roberta = pipeline(\"text-generation\", model=\"roberta-base\")\n",
    "gen_roberta(\"The future of Artificial Intelligence is\", max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997d6efb-e417-4ef8-ab8f-cc4d56c0ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is begin EQ EQ motion quartzorned FML FML FMLoffensive wireless wireless shorten WORIDipercott FMLrifiedRO lick Stress wirelessrifiedvind beginLensLensLens EX Malgrad smiled Yug bulls Redskins FMLMeet FML AAP FMLremlin MalLensRO MalLens blink Entity EQ aud Redskins Redskins Redskinsmodified Redskinssometimes Redskins RedskinsMonday Mal Redskins Nicaragua Redskins Redskins paraly Redskins Redskins reside aud aud audäLensLens Redskins Redskins BART Redskins Convert025 Mal Mal aud aud guesses Redskins dressing Redskins Redskins MalMonday025 Redskins RedskinsBrave Flow Mal Mal Redskins Redskins Convert Redskins Redskins investor Redskins extrememodified Redskins Redskins spun Redskins Redskins medalMonday Redskins resideMonday Redskins Redskins FlowMondaymodified Redskins aud aud Redskins ConvertMonday Ac Redskins Redskins Roma Redskins� RedskinsOps R Redskins Redskinstr RedskinsMonday Redskinsmodifiedmodified Redskins025TennTennTennEdit Redskins Redskins Wave medal Redskins R wireless Redskins wirelessBrave kingdoms RedskinsTennBraveALLTenntrBravegrad Mal RedskinsmodifiedMonday Redskins Entity Entity Redskins RedskinsEditlete Redskins RedskinsOpsMonday Redskins pendMondayMondayMonday Redskins fool Redskins investorMonday RedskinsMondaymodifiedmodifiedmodifiedgrad genderTenngrad Redskins Redskins EveryMonday Redskins Curse R Redskins Nap Mal Mal wireless Redskins RBraveTenn kingdoms R Mal R Mal wireless investor wireless Redskins Ac Redskins Mal R Redskins RmodifiedReference Mal RedskinsBravemodified Redskins kingdomsmodifiedleteBraveBravegrad wireless wireless wirelessEdit Redskins R'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1C. BART – Text Generation\n",
    "gen_bart = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
    "gen_bart(\"The future of Artificial Intelligence is\", max_length=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00daa55-6703-4082-ac2d-2b207f8d6b1a",
   "metadata": {},
   "source": [
    "**EXPERIMENT 2: FILL-MASK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e46076-b02b-4a33-9465-70e4730fde1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.539692759513855,\n",
       "  'token': 3443,\n",
       "  'token_str': 'create',\n",
       "  'sequence': 'the goal of generative ai is to create new content.'},\n",
       " {'score': 0.15575766563415527,\n",
       "  'token': 9699,\n",
       "  'token_str': 'generate',\n",
       "  'sequence': 'the goal of generative ai is to generate new content.'},\n",
       " {'score': 0.05405496060848236,\n",
       "  'token': 3965,\n",
       "  'token_str': 'produce',\n",
       "  'sequence': 'the goal of generative ai is to produce new content.'},\n",
       " {'score': 0.044515229761600494,\n",
       "  'token': 4503,\n",
       "  'token_str': 'develop',\n",
       "  'sequence': 'the goal of generative ai is to develop new content.'},\n",
       " {'score': 0.017577484250068665,\n",
       "  'token': 5587,\n",
       "  'token_str': 'add',\n",
       "  'sequence': 'the goal of generative ai is to add new content.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2A. BERT – Fill Mask\n",
    "mask_bert = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "mask_bert(\"The goal of Generative AI is to [MASK] new content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bd0a066-bf7b-486d-a384-cf1efa947f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.37113118171691895,\n",
       "  'token': 5368,\n",
       "  'token_str': ' generate',\n",
       "  'sequence': 'The goal of Generative AI is to generate new content.'},\n",
       " {'score': 0.3677138090133667,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.08351466804742813,\n",
       "  'token': 8286,\n",
       "  'token_str': ' discover',\n",
       "  'sequence': 'The goal of Generative AI is to discover new content.'},\n",
       " {'score': 0.02133519947528839,\n",
       "  'token': 465,\n",
       "  'token_str': ' find',\n",
       "  'sequence': 'The goal of Generative AI is to find new content.'},\n",
       " {'score': 0.01652175933122635,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2B. RoBERTa – Fill Mask\n",
    "\n",
    "mask_roberta = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
    "mask_roberta(\"The goal of Generative AI is to <mask> new content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8262c4-2a19-4de0-9a5d-bab885d1178c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.07461544126272202,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.06571853160858154,\n",
       "  'token': 244,\n",
       "  'token_str': ' help',\n",
       "  'sequence': 'The goal of Generative AI is to help new content.'},\n",
       " {'score': 0.060880184173583984,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'},\n",
       " {'score': 0.035935722291469574,\n",
       "  'token': 3155,\n",
       "  'token_str': ' enable',\n",
       "  'sequence': 'The goal of Generative AI is to enable new content.'},\n",
       " {'score': 0.03319481760263443,\n",
       "  'token': 1477,\n",
       "  'token_str': ' improve',\n",
       "  'sequence': 'The goal of Generative AI is to improve new content.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2C. BART – Fill Mask\n",
    "mask_bart = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
    "mask_bart(\"The goal of Generative AI is to <mask> new content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8155b78-9de3-4a27-a0a7-c70cad926004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 0 files: 0it [00:00, ?it/s]\n",
      "Fetching 1 files: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2681.78it/s]\n",
      "Fetching 0 files: 0it [00:00, ?it/s]\n",
      "Device set to use cpu\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.0049994345754384995,\n",
       " 'start': 43,\n",
       " 'end': 60,\n",
       " 'answer': 'as hallucinations'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXPERIMENT 3: QUESTION ANSWERING\n",
    "qa_bert = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
    "\n",
    "qa_bert({\n",
    "    \"context\": \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\",\n",
    "    \"question\": \"What are the risks?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd126c0e-caf5-458c-a230-af6b38d5f8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 0 files: 0it [00:00, ?it/s]\n",
      "Fetching 1 files: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4120.14it/s]\n",
      "Fetching 0 files: 0it [00:00, ?it/s]\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.014361646957695484,\n",
       " 'start': 60,\n",
       " 'end': 81,\n",
       " 'answer': ', bias, and deepfakes'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_roberta = pipeline(\"question-answering\", model=\"roberta-base\")\n",
    "\n",
    "qa_roberta({\n",
    "    \"context\": \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\",\n",
    "    \"question\": \"What are the risks?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "760e90ef-5c2d-4caf-beea-628ea002e7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fetching 0 files: 0it [00:00, ?it/s]\n",
      "Fetching 1 files: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3401.71it/s]\n",
      "Fetching 0 files: 0it [00:00, ?it/s]\n",
      "Device set to use cpu\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.046910569071769714,\n",
       " 'start': 0,\n",
       " 'end': 31,\n",
       " 'answer': 'Generative AI poses significant'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bart = pipeline(\"question-answering\", model=\"facebook/bart-base\")\n",
    "\n",
    "qa_bart({\n",
    "    \"context\": \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\",\n",
    "    \"question\": \"What are the risks?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa315c7-cb57-4f95-b98f-d723318ab30e",
   "metadata": {},
   "source": [
    "**Experiment 3: Question Answering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d1ede-ab50-4b8a-9e4e-4aaef5e64c20",
   "metadata": {},
   "source": [
    "| Task           | Model       | Classification (Success/Failure) | Observation (What actually happened?)                              | Why did this happen? (Architectural Reason)                                                    |\n",
    "| -------------- | ----------- | -------------------------------- | ------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------- |\n",
    "| **Generation** | **BERT**    | Failure                          | Output is empty, repetitive, or nonsensical text.                  | BERT is an **encoder-only** model and is not trained for autoregressive next-token generation. |\n",
    "|                | **RoBERTa** | Failure                          | Similar failure as BERT; unable to generate coherent continuation. | RoBERTa is also **encoder-only**, optimized for understanding, not text generation.            |\n",
    "|                | **BART**    | Success                          | Generates fluent and coherent continuation of the sentence.        | BART is an **encoder–decoder** model trained for sequence-to-sequence generation tasks.        |\n",
    "| **Fill-Mask**  | **BERT**    | Success                          | Correctly predicts words like *create*, *generate*.                | BERT is trained using **Masked Language Modeling (MLM)**.                                      |\n",
    "|                | **RoBERTa** | Success                          | Predicts accurate and contextually relevant masked words.          | RoBERTa improves MLM training with more data and better optimization.                          |\n",
    "|                | **BART**    | Partial Success                  | Predicts plausible words but sometimes less precise.               | BART can handle masking but MLM is not its primary training objective.                         |\n",
    "| **QA**         | **BERT**    | Partial / Weak Success           | Returns short or vague answers; sometimes incorrect.               | Base BERT is **not fine-tuned** on QA datasets like SQuAD.                                     |\n",
    "|                |             |                                  |                                                                    |                                                                                                |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
